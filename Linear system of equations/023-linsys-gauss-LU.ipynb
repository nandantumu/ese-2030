{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7554ca-5ac4-4958-b713-648e3fd54114",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: Gaussian Elimination Revisited\n",
    "subject:  Linear Algebraic Systems\n",
    "subtitle: Gaussian Elimination as Matrix Factorization\n",
    "short_title: LU Factorization\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Gaussian Elimination, LU factorization\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca8556-3e7f-4f1c-b466-04f0e14b12e5",
   "metadata": {},
   "source": [
    "## Reading\n",
    "Material related to this page, as well as additional exercises, can be found in LAA Ch. 2.5, ALA Ch 1.3, and ILA Ch. 2.6.  This page is mostly based on ALA Ch 1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170438e7",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- what the $LU$ factorization of a matrix is\n",
    "- how to apply $LU$ factorization to solve systems of linear equations\n",
    "- how this approach relates to Gaussian Elimination (forward elimination and back substitution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6aeff-4ad9-42b7-9fb4-a82ada3e07b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Gaussian Elimination: Regular Case\n",
    "With basic matrix arithmetic operations in our toolkit, we will develop a systematic method for solving linear systems of equations.  For a linear system $A\\vv x = \\vv b$, with $A$ an $m\\times n$ coefficient matrix, $\\vv x$ an $n \\times 1$ unknowns vector, and $\\vv b$ an $m \\times 1$ right hand side vector, we define the _augmented matrix_:\n",
    "```{math}\n",
    ":label: augmat\n",
    "M = \\left[\\begin{array}{c|c} A & \\vv b \\end{array}\\right]\n",
    "=\\left[ \\begin{array}{cccc|c} \n",
    "a_{11} & a_{12} & \\cdots & a_{1n} & b_1 \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} & b_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn} & b_m \\end{array}\\right],\n",
    "```\n",
    "which is an $m \\times (n+1)$ matrix obtained by tacking the right-hand side vector $\\vv b$ onto the right of the coefficient matrix $A$.  The extra vertical line is just to remind us that the last column of this matrix plays a special role.  For example, the augmented matrix for our [example system](./021-linsys-gauss.ipynb#simple-linsys) is\n",
    "\\begin{equation}\n",
    "\\label{augmat-ex}\n",
    "M = \\left[ \\begin{array}{ccc|c} 1 & 2 & 1 & 2\\\\ 2 & 6 & 1 & 7 \\\\ 1 & 1 & 4 & 3 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "Note that it is simple to go back and forth between the original linear system and the augmented matrix, but since operations on equations also affect their right-hand sides, it is convenient to keep track of everything together using the augmented matrix.\n",
    "\n",
    "For the time being, we will concentrate our efforst on linear systems that have the same number, $n$, of equations as unknowns.  The associated coefficient matrix $A$ is square of size $n \\times n$, and the corresponding augmented matrix $M = [ A \\, | \\, \\vv b]$ then has size $n \\times (n+1)$.\n",
    "\n",
    "We start with a simple observation connecting [Linear System Operation \\#1](./021-linsys-gauss.ipynb#linop1) to its equivalent matrix operation\n",
    "```{prf:observation} Elementary Row Operation \\#1\n",
    ":label: rowop1\n",
    "Adding a scalar multiple of one row of the augmented matrix to another row is the equivalent of adding a multiple of one equation to another in the system of linear equations it defines.  As such, this does not change the solution set and leads to an equivalent augmented matrix.\n",
    "```\n",
    "\n",
    "For example, when solving [example system](./021-linsys-gauss.ipynb#simple-linsys), our first step was to subtract two times the first equation from the second.  This is equivalently done by subtracting two times the first row of the augmented matrix [](#augmat-ex) from the second row:\n",
    "$$\n",
    "-2\\bm 1 & 2 & 1 & 2 \\em + \\bm 2 & 6 & 1 & 7 \\em = \\bm 0 & 2 & -1 & 3\\em.\n",
    "$$\n",
    "We recognize this as the second row of the modified augmented matrix\n",
    "\\begin{equation}\n",
    "\\label{pivot1}\n",
    "\\left[ \\begin{array}{ccc|c} 1 & 2 & 1 & 2\\\\ 0 & 2 & -1 & 3\\\\ 1 & 1 & 4 & 3 \\end{array}\\right],\n",
    "\\end{equation}\n",
    "that corresponds to the [first equivalent example system](./021-linsys-gauss.ipynb#simple-linsys0).  When elementary row operation \\#1 is performed, it is critical that the result replaces the row being added to and _not_ the row being multiplied by the scalar.  Notice that the elimination of a variable in an equation, in this case the first variable in the second equation, amounts to making its entry in the coefficient matrix equal to zero.\n",
    "\n",
    "### Pivots\n",
    "```{image} ../figures/02-pivot.gif\n",
    ":alt: Pivot!\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "We will call the $(1,1)$ entry of the coefficient matrix the _first pivot_.  The precise definition of a pivot will become clear as we continue, but one key requirement is that _a pivot must always be nonzero_.  Eliminating the first variable $x_1$ from teh second and third equations is the same as making all of the matrix entries in the column below the pivot equal to zero.  We have already done this with the $(2,1)$ entry in [](#pivot1).  To make the $(3,1)$ entry equal to zero, we subtract the first from from the last row, resulting in the augmented matrix\n",
    "\\begin{equation}\n",
    "\\label{pivot2}\n",
    "\\left[ \\begin{array}{ccc|c} 1 & 2 & 1 & 2\\\\ 0 & 2 & -1 & 3\\\\ 0 & -1 & 3 & 1 \\end{array}\\right],\n",
    "\\end{equation}\n",
    "which we again recognize as the corresponding to the [second equivalent example system](./021-linsys-gauss.ipynb#simple-linsys1).  The _second pivot_ is the $(2,2)$ entry of this matrix, which is $2$, and is hte coefficient of the second variable $x_2$ in the second equation.  Again, the pivot must be nonzero.  We use the [](#rowop1) of adding $1/2$ of the second row to the third row to make the entry below the second pivot equal to 0, resulting in the augmented matrix\n",
    "\\begin{equation}\n",
    "\\label{pivot3}\n",
    "\\left[ \\begin{array}{ccc|c} 1 & 2 & 1 & 2\\\\ 0 & 2 & -1 & 3\\\\ 0 & 0 & \\frac{5}{2} & \\frac{5}{2} \\end{array}\\right],\n",
    "\\end{equation}\n",
    "that corresponds to the [triangular system equivalent to our example system](./021-linsys-gauss.ipynb#simple-linsys2).  We write the final augmented matrix as\n",
    "$$\n",
    "N = [U \\, | \\, \\vv c], \\quad \\text{where} \\quad U = \\bm 1 & 2 & 1 \\\\ 0 & 2 & -1 \\\\ 0 & 0 & \\frac{5}{2}\\em, \\quad \\vv c = \\bm 2 \\\\ 3 \\\\ \\frac{5}{2} \\em.\n",
    "$$\n",
    "\n",
    "The corresponding linear system can be written as $U\\vv x = \\vv c$.  A special feature of this system is that the coefficient matrix $U$ is _upper triangular_[^upper], which means that all entries below the main diagonal are zero, i.e., $u_{ij}=0$ whenever $i>j$.  The three nonzero entries on its diagonal, $1$, $2$, and $5/2$, including the last one in the $(3,3)$ slot, are the three pivots.  Once the system has been reduced to this triangular form, we can easily solve it via Back Substitution.\n",
    "\n",
    "[^upper]: It's convention we used the symbol $U$ to remind ourselves that the matrix is upper triangular.\n",
    "\n",
    "What we just described is an algorithm for solving a linear system of $n$ equations in $n$ unknowns, and is known as _regular Gaussian Elimination_.  We'll call a square matrix $A$ _regular_ if the algorithm successfully reduces it to the upper triangular form $U$ with all nonzero pivots on the diagonal.  If this fails to happen, i.e., if a pivot appearing on the diagonal is zero, then the matrix is not regular.  We then use the pivot row to make all entries lying in the column below the pivot equal to zero through elementary row operations.  The solution is then found by applying Back Substitution to the resulting system.  We'll summarize both of these algorithms in _pseudocode_ below.  Later, we'll see how to translate this pseudocode into actual Python code that can be run on a computer.\n",
    "\n",
    ":::{prf:algorithm} Regular Gaussian Elimination\n",
    ":label: reg-ge\n",
    "\n",
    "**Inputs** Augmented matrix $M = [ A \\, | \\, \\vv b]$\n",
    "\n",
    "**Output** Equivalent upper triangular form $M = [U \\, | \\, \\vv c]$ if $A$ is regular, \"$A$ is not regular\" token otherwise\n",
    "\n",
    "for $j=1$ to $n$:\\\n",
    "$\\quad$ if $m_{jj}=0$:\\\n",
    "$\\quad \\quad$ **return** \"$A$ is not regular\"\\\n",
    "$\\quad$ else for $i= j + 1$ to $n$:\\\n",
    "$\\quad \\quad$ set $l_{ij}\\leftarrow m_{ij}/m_{jj}$\\\n",
    "$\\quad \\quad$ add $-l_{ij}$ times row $j$ of $M$ to row $i$ of $M$\\\n",
    "**return** $M = [U \\, | \\, \\vv c]$ \n",
    ":::\n",
    "Here we use what are called _in place updates_, meaning that the same letter $M$ (with entries $m_{ij}$) denotes the current augmented matrix at each stage in the computation.  We initialize with $M=[A \\, | \\, \\vv b]$, and output (assuming $A$ is regular) the upper triangular equivalent augmented matrix $M = [U \\, | \\, \\vv c]$, where $U$ is the upper triangular matrix with diagonal entries the pivots, and $\\vv c$ is the resulting vector of the right-hand sides of the triangular system $U\\vv x = \\vv c$.\n",
    "\n",
    "Next, let's take a look at the pseudocode for Back Substitution.\n",
    ":::{prf:algorithm} Back Substitution\n",
    ":label: back-sub\n",
    "\n",
    "**Inputs** Triangular form augmented matrix $M = [U \\, | \\, \\vv c]$.  $U$ is assumed to have nonzero diagonals $u_{ii}\\neq 0$.\n",
    "\n",
    "**Output** Solution $\\vv x$ to $U\\vv x = \\vv c$.\n",
    "\n",
    "set $x_n\\leftarrow c_n/u_{nn}$\\\n",
    "for $i=n-1$ to $1$: (decrementing by $-1$ at each iteration)\\\n",
    "$\\quad$ set $x_i \\leftarrow \\frac{1}{u_{ii}}\\left(c_i-\\displaystyle\\sum_{j=1}^{i+1}u_{ij}x_j\\right)$\\\n",
    "**return** solution $\\vv x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6746a76-78aa-42fd-b3ac-322ad879d1ae",
   "metadata": {},
   "source": [
    "### Worked Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceffd52-56eb-4625-8fbc-e4696a69e3f4",
   "metadata": {},
   "source": [
    "````{exercise}  TODO\n",
    ":label: row-reduce-ex1\n",
    "Write me\n",
    ":::{hint} Click me for a hint!\n",
    ":class: dropdown\n",
    "Write me\n",
    "\n",
    ":::\n",
    "```{solution} my-exercise\n",
    ":class: dropdown\n",
    "Write me\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6493b5-6b12-415a-a159-646579661219",
   "metadata": {},
   "source": [
    "````{exercise}  TODO\n",
    ":label: row-reduce-ex1\n",
    "Write me\n",
    ":::{hint} Click me for a hint!\n",
    ":class: dropdown\n",
    "Write me\n",
    "\n",
    ":::\n",
    "```{solution} my-exercise\n",
    ":class: dropdown\n",
    "Write me\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40ebc5-ca2e-4efb-9fa7-67b860d67463",
   "metadata": {},
   "source": [
    "## Elementary Matrices\n",
    "It turns out that our strategy of adding equations together is in fact an operation that can be realized by matrix multiplication.  Our starting point will be to introduce a the idea of an _elementary matrix_, which encodes the corresponding elementary operation.  So far we've only seen the elementary operation of adding equations together, which we recall, is equivalent to adding rows of the augmented matrix $M = [A \\, | \\, \\vv b]$ together, but we'll see other types of operations shortly.  The definition below will work for all of them.\n",
    "\n",
    "```{prf:definition} Elementary Matrix\n",
    ":label: elementary\n",
    "The _elementary matrix_ associated with an elementary row operation for $m$-rowed matrices is the $m \\times m$ matrix obtained by applying the the row operation to the $m\\times m$ identity matrix $I_m$.\n",
    "```\n",
    "\n",
    "This definition is a bit abstract, so let's see it in action.  Suppose that we have a system of three equations in three unknowns, encoded by the augmented matrix $M = [ A\\, | \\, \\vv b]$, where $A$ is a $3\\times 3$ matrix and $\\vv b$ is a $3 \\times 1$ vector.  What is the elementary matrix associated with subtracting twice the first row from teh second row?  If we start with the identify matrix \n",
    "$$\n",
    "I_3 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\em,\n",
    "$$\n",
    "and apply this operation to it, we end up with the elementary matrix \n",
    "$$\n",
    "E_1 = \\bm 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1\\em,\n",
    "$$\n",
    "which we named $E_1$ just to keep track of which elementary matrix we are talking about later on.  Let's check if it does what it's supposed to on a two different $3 \\times 3$ matrices \n",
    "$$\n",
    "\\underbrace{\\bm 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1\\em}_{E_1}\\bm 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\em = \\bm 1 & 2 & 3 \\\\ 2 & 1 & 0 \\\\ 7 & 8 & 9 \\em, \\quad \\underbrace{\\bm 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1\\em}_{E_1}\\bm 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\em =\\bm 1 & 2 & 3 \\\\ -1 & -2 & -3 \\\\ 1 & 2 & 3 \\em .\n",
    "$$\n",
    "Indeed it does, and by playing around with our rules of matrix arithmetic and multiplication, you should be able to convince yourself that that left multiplying _any_ 3-row matrix by $E_1$ will subtract twice its from its second row.\n",
    "\n",
    "We can also use [](#elementary) to reverse engineer what row operation an elementary matrix is encoding.\n",
    "\n",
    "````{exercise} Reverse Engineering Elementary Matrices\n",
    ":label: em-ex\n",
    "What elementary row operations are realized by left multiplying by the following matrices?\n",
    "$$\n",
    "E_2 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\em, \\quad E_3 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\em.\n",
    "$$\n",
    ":::{hint} Click me for a hint!\n",
    ":class: dropdown\n",
    "What would I have to do to\n",
    "$$\n",
    "I_3 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\em,\n",
    "$$\n",
    "to get $E_2$ and $E_3$?\n",
    ":::\n",
    "```{solution} em-ex\n",
    ":class: dropdown\n",
    "Left multiplying by $E_2$ realizes subtracting the first row from the last row of a matrix.  This is true because to get the last row of $E_2$, $\\bm -1 & 0 & 1\\em$ from $I_3$, we need to subract the first row $\\bm 1 & 0 & 0 \\em$ of $I_3$ from the last row $\\bm 0 & 0 & 1 \\em$ of $I_3$.\n",
    "\n",
    "Using similar reasoning, we see that left multiplying by $E_3$ realizes adding $1/2$ the second row of a matrix to its last row.\n",
    "```\n",
    "````\n",
    "\n",
    "### Elementary Matrices in Action\n",
    "Let us use elementary matrices to design a matrix that can help us solve linear equations.  Let's revisit the very first system of equations we encountered:\n",
    "\\begin{eqnarray}\n",
    "\\label{simple-linsys}\n",
    "x_1+2x_2+x_3 & = 2,\\\\\n",
    "2x_1+6x_2+x_3 & =7,\\\\\n",
    "x_1+x_2+4x_3 & =3,\n",
    "\\end{eqnarray}\n",
    "or in matrix-vector form\n",
    "$$\\underbrace{\\begin{bmatrix} 1 & 2 & 1 \\\\\n",
    "2 & 6 & 1 \\\\\n",
    "1 & 1 & 4 \\end{bmatrix}}_A\\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}}_{\\vv x} = \\underbrace{\\begin{bmatrix} 2 \\\\ 7 \\\\ 3 \\end{bmatrix}}_{\\vv b}\n",
    "$$\n",
    "which we solved by first applying the row operation encoded by $E_1$ (subtracted twice equation 1 from equation 2), then applying the operation encoded by $E_2$ (subtract the first equation from the last equation ), and finally by applyiing the operation encoded by $E_3$ (add 1/2 first equation to last equation).  Funny how that conveninently worked out!  Therefore, if we keep careful track of the row operations and their matrix multiplication realizations, we conclude that:\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 & 1 \\\\\n",
    "2 & 6 & 1 \\\\\n",
    "1 & 1 & 4 \\end{bmatrix}, \\quad E_3 E_2 E_1 A = U = \\bm 1 & 2 & 1 \\\\ 0 & 2 & -1 \\\\ 0 & 0 & \\frac{5}{2} \\em.\n",
    "$$\n",
    "The way to read the expression $E_3 E_2 E_1 A$ is from left-to-right: we start with $A$, then apply $E_1$, then $E_2$, and finally $E_3$.  If we give the composition of elementary operations a name, say $E = E_3 E_2 E_1 A$, then we can check that $E [A \\, | \\, \\vv b] = [U \\, | \\, \\vv c]$, that is to say left multiplying the augmented matrix $M = [A \\, | \\, \\vv b]$ by the matrix $E$ performs Gaussian Elmination for us!  This is just a first taste of a powerful feature of linear algebra: _we can encode very sophisticated and complex operations via matrix multiplication._\n",
    "```{important}\n",
    "The order of the operations here matters!  For example, $E_2 E_1 E_3 A$ is a very different matrix than $E_3 E_2 E_1 A$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c13f7-bef5-4b4a-9b35-1ed79560f3aa",
   "metadata": {},
   "source": [
    "### Solving Linear Equations with $LU$-factorizations\n",
    "We've made a lot of progress, but we still do not have a way of dealing with cases where the matrix $A$ is not regular, i.e., when it has pivots that are zero.  We'll defer that situation for a little bit still, and instead look at why we bothered with $LU$-factorizations to begin with!  Suppose we are interested in solving a system of linear equations $A\\vv x = \\vv b$, and we assume that $A$ is regular so that we can apply the algorithm above to compute an $LU$-facotrization of $A$, i.e., we find a lower triangular matrix $L$ and an upper triangular matrix $U$ so that $A=LU$.  Then our linear system becomes $LU\\vv x = \\vv b$.  \n",
    "\n",
    "Neat, but so what?  Well we know how to solve linear systems that look like $U\\vv x = \\vv z$, for $U$ upper triangular, by Back Substitution, and while we haven't seen it explicitly, hopefully you believe that we can solve linear systems that look like $L\\vv y = \\vv c$, for $L$ lower triangular, by _Forward Substitution_.[^fsub]  Our strategy will be to turn solving $A\\vv x = \\vv b$ into solving a lower triangular system by Forward Substitution, and then an upper triangular system by Back Substitution, both things we know how to do easily!  With that in mind, let us introduce a new intermediate variable $\\vv z = U \\vv x$.  Then I can find the solution to $A\\vv x = LU\\vv x = \\vv b$ by solving\n",
    "$$\n",
    "L\\vv z = \\vv b,\n",
    "$$\n",
    "for $\\vv z$ via Forward Substitution, and then solving \n",
    "$$\n",
    "U \\vv x = \\vv z,\n",
    "$$\n",
    "for $\\vv x$ via Backward Substitution.  If we piece everything together, we see that $\\vv x$ is indeed a solution to our original equation $A\\vv x = \\vv b$ because\n",
    "$$\n",
    "A\\vv x = L\\underbrace{U \\vv x}_{=\\vv z}= \\underbrace{L\\vv z}_{=\\vv b} = \\vv b.\n",
    "$$\n",
    "\n",
    "[^fsub]: Forward Substitution is exactly what it sounds like.  To solve $L\\vv y = \\vv c$, when $L$ is lower triangular, we start at the top row and set $y_1 = c_1/l_{11}$. We then move down a row, and solve $l_{21}y_1 + l_{22}y_2 = c_2$ by plugging in $y_1$ and solving for $y_2$.  We continue in this manner until we've solved for $\\vv y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3acfc-93ea-42bf-9d2f-53d30b2d6c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
